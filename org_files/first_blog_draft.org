#+Title: Unifying Reifying and Symbolic-PyMC
#+Author: Joseph Willard
#+Date: 2019-10-06

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: ^:nil toc:nil d:(not "logbook" "todo" "note" "notes") tex:t |:t broken-links:mark
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+PROPERTY: header-args :session tf :exports both :eval never-export :results output drawer replace
#+PROPERTY: header-args:text :eval never
#+OPTIONS: toc:nil

* Introduction
Digging through TensorFlow I started by computing basic examples and
comparing them to numpy's output. While doing this I came across the
common theme of numerical approximation that theoretically should not
have been present. Of course this brought me to pondering what would I
have to do to get around these numerical errors that arise in software
today?

In this article consider the situation were one passingly uses an SVD.

#+BEGIN_SRC python -n :results value table
  import numpy as np

  X = np.random.normal(0, 1, (10, 10))
  S = X.T.dot(X)
  U, d, Vt = np.linalg.svd(S)
  _ = S - np.dot(U*d, Vt)
#+END_SRC

#+RESULTS:
:RESULTS:
|  7.10542736e-15 | -1.15463195e-14 | -2.66453526e-15 |  1.24344979e-14 |  2.22044605e-15 | -6.66133815e-16 |  1.19904087e-14 |  -4.6629367e-15 |  3.33066907e-16 |   4.4408921e-15 |
| -1.28785871e-14 |  5.32907052e-15 |  5.32907052e-15 | -5.32907052e-15 | -1.77635684e-15 |   8.8817842e-16 | -1.52655666e-14 |  1.77635684e-15 |   8.8817842e-15 | -3.55271368e-15 |
|  1.24344979e-14 | -7.10542736e-15 |  -8.8817842e-15 |  1.77635684e-15 |  -8.8817842e-16 | -1.77635684e-15 |  8.43769499e-15 | -2.22044605e-15 | -2.66453526e-15 |  6.21724894e-15 |
|   8.8817842e-15 | -1.64313008e-14 |  7.10542736e-15 | -1.77635684e-15 | -6.21724894e-15 |  -4.4408921e-16 |  5.32907052e-15 | -6.66133815e-15 |  2.22044605e-16 | -2.44249065e-15 |
|  -4.4408921e-16 |             0.0 |  1.44328993e-15 |  -4.4408921e-15 | -1.77635684e-15 | -7.42461648e-16 | -1.99840144e-15 |  1.11022302e-15 |  2.22044605e-15 | -1.77635684e-15 |
| -2.06501483e-14 |  1.66533454e-14 |  1.59872116e-14 | -9.76996262e-15 |  6.52256027e-16 |             0.0 | -1.33226763e-14 |   4.4408921e-15 |  5.77315973e-15 | -7.10542736e-15 |
| -1.99840144e-14 |  1.06026299e-14 |   1.7985613e-14 | -7.10542736e-15 |  -8.8817842e-16 |  3.99680289e-15 | -1.42108547e-14 |  2.66453526e-15 |   4.4408921e-15 | -1.27675648e-14 |
|  5.55111512e-15 | -2.66453526e-15 | -7.10542736e-15 |  1.77635684e-15 |  6.66133815e-16 |             0.0 |   4.4408921e-16 |  -8.8817842e-16 | -7.07767178e-16 |  2.66453526e-15 |
|  3.33066907e-14 | -2.39808173e-14 | -2.04281037e-14 |  1.17683641e-14 |  -8.8817842e-16 | -3.99680289e-15 |  2.66453526e-14 | -7.91033905e-15 |  -1.0658141e-14 |  1.37667655e-14 |
|  2.23154828e-14 | -1.42108547e-14 | -1.77635684e-14 |  1.02140518e-14 |  1.33226763e-15 |             0.0 |  1.44051437e-14 | -5.32907052e-15 | -7.10542736e-15 |  7.10542736e-15 |
:END:

Let's see if TensorFlow exhibits the same issue?

#+BEGIN_SRC python -n :results value table
  """ Seeing if tensorflow has the same issue
  """
  import tensorflow as tf
  from tensorflow.python.framework.ops import disable_eager_execution


  tf.compat.v1.InteractiveSession()
  disable_eager_execution()

  tfp = tfp.distributions

  X = np.random.normal(0, 1, (10, 10))

  S = tf.matmul(X, X, transpose_a=True)

  d, U, V = tf.linalg.svd(S)

  D = tf.matmul(U, tf.matmul(tf.linalg.diag(d), V, adjoint_b=True))
  ans = S - D

  _ = ans.eval()
#+END_SRC

#+RESULTS:
:RESULTS:
| -3.01980663e-14 |  -4.4408921e-15 |  2.39808173e-14 |   4.4408921e-15 |  7.99360578e-15 |  -2.7533531e-14 |  1.37667655e-14 | -1.59872116e-14 |  2.48689958e-14 |  7.10542736e-15 |
| -5.99520433e-15 | -1.24344979e-14 |  6.88338275e-15 | -1.24344979e-14 |  1.77635684e-15 | -1.82076576e-14 | -1.66533454e-15 | -5.77315973e-15 | -3.99680289e-15 | -1.95399252e-14 |
|  2.13162821e-14 |  2.88657986e-15 | -1.77635684e-14 |  2.22044605e-15 | -7.99360578e-15 |  2.57571742e-14 | -1.02140518e-14 |  5.88418203e-15 | -1.55431223e-14 |  3.33066907e-16 |
|  5.77315973e-15 | -1.77635684e-14 |  2.22044605e-16 | -1.77635684e-14 | -1.94289029e-15 |  -1.0658141e-14 |  -8.8817842e-15 |  4.99600361e-15 | -2.66453526e-15 | -2.13162821e-14 |
|  7.77156117e-15 |  1.77635684e-15 |  -4.4408921e-15 | -1.22124533e-15 | -7.99360578e-15 |  1.46549439e-14 | -4.08006962e-15 | -1.99840144e-15 |  -1.0658141e-14 |  1.55431223e-15 |
| -2.84217094e-14 |  -1.9095836e-14 |  2.66453526e-14 |  -8.8817842e-15 |  1.66533454e-14 | -4.08562073e-14 |  3.55271368e-15 | -7.77156117e-16 |  3.01980663e-14 | -1.59872116e-14 |
|  1.28785871e-14 | -1.88737914e-15 |  -1.0658141e-14 |  -8.8817842e-15 |  -3.1918912e-15 |  -8.8817842e-16 | -4.97379915e-14 |  3.90798505e-14 |  1.19904087e-14 | -3.55271368e-14 |
| -1.59872116e-14 | -5.32907052e-15 |  9.43689571e-15 |  5.32907052e-15 | -6.66133815e-16 | -2.44249065e-15 |  3.37507799e-14 | -2.48689958e-14 | -1.15463195e-14 |   1.0658141e-14 |
|  2.30926389e-14 | -7.77156117e-16 | -1.28785871e-14 |  -8.8817842e-16 | -7.10542736e-15 |  2.57571742e-14 |  8.43769499e-15 | -1.24344979e-14 | -3.55271368e-14 |  1.15463195e-14 |
|  4.88498131e-15 | -2.13162821e-14 | -2.22044605e-15 | -1.86517468e-14 |  3.77475828e-15 | -1.77635684e-14 | -3.73034936e-14 |  1.59872116e-14 |  1.50990331e-14 | -5.32907052e-14 |
:END:

In regards to theory this should have been 0, but due to rounding
errors mostly drawn from limitations of floats this is not the
case. Questions one might ask are "Is there a way around this?"
and "Why would one care?"

Answering the second question, one reason is of course optimizing
numerical deficiencies when possible. The other could be this idea of
automating "pen and paper" math. This would allow someone with a
domain specific skill set be it in probability, numerical analysis to
be able to automate their "tricks" and demystify more complex ideas in
their respective fields.

Moving back to the first question, one method is to think of the
process of doing SVD above as a graph of operations. In this graph
each node is the output of an operation which are represented as the
edge connecting nodes. What this would allow us to do is to traverse
the graph looking for operations that could be reduced or removed all
together.


** An analogy
Consider an analogy using strings,

#+BEGIN_SRC python
input_src_code = """
u, d, v = svd(S)
ans = S - matmul(u, d, v)
"""
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

Consider some ~optimize~ function, that does the following:

#+BEGIN_SRC python
output_src_code = str_optimize(input_src_code)
#+END_SRC

where

#+BEGIN_SRC python
assert output_src_code == """
u, d, v = svd(S)
ans = S - S
"""
#+END_SRC

In this we need to replace ~"matmul(u, d, v)"~ with ~"S"~, but what
do we need in order to implement this? 

1. Match the pattern for an SVD, e.g.
  #+BEGIN_SRC python
    import re
    res = re.search("([a-zA-Z]+), ([a-zA-Z]+), ([a-zA-Z]+) = svd\(([a-zA-Z]+)\)", input_src_code)
    U = res.group(1)
    D = res.group(2)
    V = res.group(3)
    S = res.group(4)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

2. If it matches, match and replace the "matmul", e.g. with
  #+BEGIN_SRC python
  optimized_code = input_src_code.replace("matmul({}, {}, {})".format(U, D, V), S)
  #+END_SRC

Using this analogy how does this map back to the TensorFlow objects
that we'll be working with?

* Graph reconstruction through TensorFlow

To begin answering the first question, let's look at what our term
~ans~ has as objects outside of the standard assigned objects. As a
side note, this [[https://blog.jakuba.net/2017/05/30/Visualizing-TensorFlow-Graphs-in-Jupyter-Notebooks/][blog post]] covers some of the same material.

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [i for i in dir(ans) if not i.startswith('_')]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
['OVERLOADABLE_OPERATORS',
 'consumers',
 'device',
 'dtype',
 'eval',
 'get_shape',
 'graph',
 'name',
 'op',
 'set_shape',
 'shape',
 'value_index']
#+END_src

One that immediately strikes some interest is ~ans.op~. 

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = ans.op
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
<tf.Operation 'sub' type=Sub>
#+END_src

A ~tf.Operation~ is a node in the graph corresponds to a
computation. Some of the properties included in ~tf.Operation~ are
~inputs~ and ~outputs~. These could be the arguments to the operation
and the outputs, which corresponds to "S" and "matmul(...)" for inputs
and "ans" for outputs in ~input_src_code~.

Using our analogy, the above TensorFlow operation is the subtraction
in the string ~input_src_code~.


#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [ans.op.inputs._inputs, ans.op.outputs]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
[[<tf.Tensor 'MatMul:0' shape=(10, 10) dtype=float64>,
  <tf.Tensor 'MatMul_2:0' shape=(10, 10) dtype=float64>],
 [<tf.Tensor 'sub:0' shape=(10, 10) dtype=float64>]]
#+END_src

These look like references to the previous tensors that were
subtracted to create ~ans~. Of course I can directly check this.

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [ans.op.inputs._inputs[0] == S, ans.op.inputs._inputs[1] == D]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
[True, True]
#+END_src

Great! So as a quick recap I now have a way to take the result ~ans~
and walk backwards to our original matrices. Is it possible to
determine what kind of operations are transpiring? Specifically, is it
possible to determine if there was an SVD operation? The quick answer
is "yes"! All I need to do is use the same methods I've used thus
far.

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = ans.op.inputs._inputs[1].op.inputs._inputs[0].op
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
<tf.Operation 'Svd' type=Svd>
#+END_src
 
This is like the "svd(...)" in our analogy, so the argument to this
"string operator" is ~op.inputs~.

At this point it's clear there exists a way to move through operations
and get the corresponding inputs and outputs. How do we do this using TensorFlow? We
know we would need a way to traverse a TensorFlow graph and find patterns like we
did above, which is analogous to searching strings with ~re.search~ and
replacing with ~str.replace~.

In later blog posts I'll dive into creating functions that parse this
graph and make the required replacements much like our string
analogy. This is one of the main goals of the ~symbolic-pymc~ package
I'll be working with during GSoC 2019.

* remove move to different blog                                    :noexport:

How does this analogy map to back the TensorFlow objects we have been working with? 

For example in our situation when is a person doing
a SVD? This concept can be described through the notion of
"goals". However, before introducing the idea one must talk about the
concepts of unify and reify. 



** Unify
The idea behind unify is to take two similar terms and form a
substitution which can be thought of as a mapping between variables
and values. Let's look at a few quick examples,

| Constant | Variable | Substitution |
| (4, 5)   | (x, 5)   | {x: 4}       |
| 'test'   | 'txst'   | {x: 'e'}     |

In layman's terms at this point I'm looking for effectively the set
of values that make the statement true. Below are some examples of
terms that do not unify,

| Constant | Variable | Substitution |
| (4, 5)   | (3, x)   | NA           |
| 'test'   | 'exror'  | NA           |

** Reify
Reify is the opposite operation to unify. This implies that it takes a
variable and a substitution and returns a value that contains no
variables. Below is a quick example,


| Variable | Substitution | Constant |
| (x, 10)  | {x: 5}       | (5, 10)  |
| 'mxsic'  | {x: 'u'}     | 'music'  |

** Goals and there constructors
Using the two concepts above I can now introduce the idea of a
goal. A goal is effectively a stream of substitutions which can be
demonstrated in the following example,

Given that `x is a member of both `(8, 5, 2) and `(5, 2, 9) a stream
of substitutions are {x: 5}, {x: 2}.

** Conclusion
The ability to create graphs and have goals like described allow us to
answer the questions posed at the beginning. In later posts I'll focus
on this idea as it relates to the work I am doing for GSoC.


* work                                                             :noexport:

#+BEGIN_SRC python -n :exports both :results output
  import symbolic_pymc.tensorflow
  from symbolic_pymc.unify import (ExpressionTuple, etuple, tuple_expression)
  z = tuple_expression(ans)
#+END_SRC


# add portion that talks about tuple_expression and unify against that

#+NAME:
#+BEGIN_SRC python -n :exports both :results output
  from graphviz import Digraph

  def tf_to_dot(graph):
     dot = Digraph()
     for n in graph.as_graph_def().node:
         dot.node(n.name, label=n.name)
         for i in n.input:
             dot.edge(i, n.name)
     return dot


  dgraph = tf_to_dot(ans.graph)

  dgraph.render('/tmp/graph.png', view=True)
#+END_SRC

#+CAPTION:

* Notes from Chris                                                 :noexport:
** DONE remove heading about tensorgraphs
*** It's more about reconstructing the graph. Maybe add on this?
**** Changed title
*** Also not that in tensorflow 2 op is removed (I ran this in tensorflow v1)
**** Fixed env
*** remove this statement (import tensorflow_probability as tfp) 
**** 
