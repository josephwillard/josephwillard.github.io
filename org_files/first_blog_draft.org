#+Title: Unifying Reifying and Symbolic-PyMC
#+Author: Joseph Willard
#+Date: 2019-10-06

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: ^:nil toc:nil d:(not "logbook" "todo" "note" "notes") tex:t |:t broken-links:mark
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+PROPERTY: header-args :session tf :exports both :eval never-export :results output drawer replace
#+PROPERTY: header-args:text :eval never
#+OPTIONS: toc:nil

* Introduction
Digging through tensorflow we started by computing basic examples and
comparing them to numpy's output. While doing this we found that in
many cases there was a common theme of numerical approximation that
theoretically should not have been present. Of course this brought me
to pondering what would we have to do to get around these numerical
errors that arise in software today?

In this article I'll start by walking through an example using SVD and
show the numerical errors, then I'll muse on what could be done to get
around this.

* Motivation
To start Let's look at a basic example using SVD.

#+BEGIN_SRC python -n :results value table
  import numpy as np

  X = np.random.normal(0, 1, (10, 10))
  S = X.T.dot(X)
  U, d, Vt = np.linalg.svd(S)
  _ = S - np.dot(U*d, Vt)
#+END_SRC

#+RESULTS:
:RESULTS:
| -6.21724894e-15 |  9.32587341e-15 | -1.55431223e-15 | -2.92821323e-15 |  5.32907052e-15 | -2.66453526e-15 | 1.11022302e-15 | -4.41313652e-15 | -4.88498131e-15 |   4.4408921e-16 |
|  -4.4408921e-16 | -1.24344979e-14 | -3.33066907e-16 |             0.0 | -5.32907052e-15 | -6.66133815e-16 | 7.99360578e-15 |   4.4408921e-16 | -7.10542736e-15 |  2.22044605e-16 |
| -3.33066907e-16 |  2.44249065e-15 |             0.0 |  2.22044605e-16 |  3.77475828e-15 |  5.32907052e-15 |  4.4408921e-16 |  -8.8817842e-16 |  2.66453526e-15 |  2.66453526e-15 |
| -2.30371278e-15 | -1.99840144e-15 | -3.99680289e-15 | -3.55271368e-15 | -2.22044605e-15 |  -8.8817842e-16 | 1.33226763e-15 |  -8.8817842e-16 | -3.99680289e-15 | -3.55271368e-15 |
| -1.77635684e-15 | -7.99360578e-15 |  1.55431223e-15 |  -8.8817842e-16 |  -8.8817842e-15 | -3.55271368e-15 | 2.66453526e-15 |  7.99360578e-15 |             0.0 |   4.4408921e-16 |
|  -4.4408921e-15 |  6.66133815e-16 |  6.21724894e-15 | -2.22044605e-16 | -2.66453526e-15 | -1.24344979e-14 | 2.66453526e-15 |   4.4408921e-15 | -2.22044605e-16 | -2.22044605e-16 |
|  1.72084569e-15 |  1.77635684e-15 |  1.77635684e-15 |  1.33226763e-15 |  3.10862447e-15 |  -8.8817842e-16 | -4.4408921e-15 |   8.8817842e-16 |  5.32907052e-15 |  -8.8817842e-16 |
|  9.99200722e-16 |  5.27355937e-15 |  -4.4408921e-15 |  -8.8817842e-16 |  9.76996262e-15 |  3.55271368e-15 |  4.4408921e-16 |  -1.0658141e-14 | -1.77635684e-15 | -7.54951657e-15 |
| -3.77475828e-15 |  -4.4408921e-16 |  -8.8817842e-16 | -1.77635684e-15 | -3.55271368e-15 |  -4.6629367e-15 | 2.66453526e-15 |             0.0 | -3.55271368e-15 |  4.88498131e-15 |
|  1.33226763e-15 |             0.0 |  1.77635684e-15 |  -4.4408921e-16 |  2.66453526e-15 |  1.11022302e-15 | 2.22044605e-16 | -4.88498131e-15 |   8.8817842e-16 | -6.21724894e-15 |
:END:

Theoretically, this would raise an eyebrow. Next, does TensorFlow exhibit the same issue?

#+BEGIN_SRC python -n :results value table
  """ Seeing if tensorflow has the same issue
  """
  import tensorflow as tf
  import tensorflow_probability as tfp
  from tensorflow.python.framework.ops import disable_eager_execution


  tf.compat.v1.InteractiveSession()
  #disable_eager_execution()

  tfp = tfp.distributions

  X = np.random.normal(0, 1, (10, 10))

  S = tf.matmul(X, X, transpose_a=True)

  d, U, V = tf.linalg.svd(S)

  D = tf.matmul(U, tf.matmul(tf.linalg.diag(d), V, adjoint_b=True))
  ans = S - D

  _ = ans.eval()
#+END_SRC

#+RESULTS:
:RESULTS:
| -2.30926389e-14 | -2.22044605e-14 | -1.77635684e-15 | -1.88737914e-15 |  7.10542736e-15 | -9.32587341e-15 |  2.66453526e-15 | -9.76996262e-15 | -2.87270208e-15 | -2.22738494e-15 |
| -1.86517468e-14 | -3.90798505e-14 |  6.66133815e-15 |  5.32907052e-15 | -1.22124533e-15 | -1.95399252e-14 | -2.22044605e-15 |  -3.8719028e-15 |  -4.4408921e-16 |  1.11022302e-15 |
| -3.10862447e-15 |  1.77635684e-15 | -2.48689958e-14 |  1.77635684e-15 |  1.19904087e-14 |   4.6629367e-15 |  6.21724894e-15 |  5.32907052e-15 |  2.39808173e-14 |  3.10862447e-15 |
| -2.10942375e-15 |  3.55271368e-15 | -2.22044605e-15 | -1.59872116e-14 |  1.02140518e-14 |  7.99360578e-15 |  1.11022302e-15 |  5.30825384e-15 | -1.11022302e-14 | -1.33226763e-15 |
|  4.88498131e-15 |  -8.8817842e-16 |  1.42108547e-14 |  8.43769499e-15 | -2.13162821e-14 | -6.06459327e-15 | -4.88498131e-15 | -4.88498131e-15 | -7.10542736e-15 |  1.99840144e-15 |
| -3.99680289e-15 | -1.59872116e-14 |  9.32587341e-15 |   8.8817842e-15 | -6.75848266e-15 |  -8.8817842e-15 | -3.55271368e-15 | -5.32907052e-15 |             0.0 |  6.66133815e-16 |
|   4.4408921e-16 | -3.55271368e-15 |  6.21724894e-15 |  2.22044605e-15 | -5.77315973e-15 | -3.55271368e-15 | -3.55271368e-15 |   8.8817842e-16 | -7.54951657e-15 | -1.60982339e-15 |
| -7.54951657e-15 | -2.51187959e-15 |   8.8817842e-16 |  5.64132074e-15 | -3.44169138e-15 | -2.66453526e-15 |  1.44328993e-15 |  -8.8817842e-15 | -6.21724894e-15 | -5.32907052e-15 |
| -7.99360578e-15 | -7.99360578e-15 |  2.13162821e-14 | -1.11022302e-14 |  -4.4408921e-16 |  -4.4408921e-15 | -3.99680289e-15 | -1.24344979e-14 | -1.77635684e-14 |  3.55271368e-15 |
| -3.21270788e-15 |  -8.8817842e-16 |  9.76996262e-15 | -3.55271368e-15 |  3.33066907e-15 | -2.44249065e-15 | -1.38777878e-15 | -3.55271368e-15 |             0.0 |  1.77635684e-15 |
:END:


In regards to theory this should have been 0, but due to rounding
errors mostly drawn from limitations of floats this is not the
case. Natural questions to ask is whether there is a way around this
and Why would we care about this?

Answering the second question One reason is of course numerical
accuracy. The other could be this idea of automating "pen and paper"
math. This would allow someone with a domain specific skill set be it
in probability, numerical analysis to be able to automate their
"tricks" and demystify more complex ideas in their respective fields.

Moving back to the first question one method is to think of the
process of doing SVD above as a graph of operations. In this graph
each node is the output of an operation that is the edge connecting
nodes. What this would allow us to do is to traverse the graph looking
for operations that could be reduced or removed all together. This
could be broken down into two larger questions. the first and one
we'll focus on in this blog is how do we create a graph of operations?
The second is how do we determine what we're looking at can be
reduced? 

To begin answering the first question let's look at what our term
"ans" has as objects outside of the standard assigned objects.


#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [i for i in dir(ans) if not i.startswith('_')]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
['OVERLOADABLE_OPERATORS',
 'consumers',
 'device',
 'dtype',
 'eval',
 'get_shape',
 'graph',
 'name',
 'op',
 'set_shape',
 'shape',
 'value_index']
#+END_src

Looking at the dir object we see a few objects of interest, let's
print the output of a few of them.

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [[i for i in dir(ans.graph) if not i.startswith('_')],
       [i for i in dir(ans.op) if not i.startswith('_')],
       [i for i in dir(ans.name) if not i.startswith('_')]]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
[['add_to_collection',
  'add_to_collections',
  'as_default',
  'as_graph_def',
  'as_graph_element',
  'building_function',
  'clear_collection',
  'collections',
  'colocate_with',
  'container',
  'control_dependencies',
  'create_op',
  'device',
  'finalize',
  'finalized',
  'get_all_collection_keys',
  'get_collection',
  'get_collection_ref',
  'get_name_scope',
  'get_operation_by_name',
  'get_operations',
  'get_tensor_by_name',
  'gradient_override_map',
  'graph_def_versions',
  'is_feedable',
  'is_fetchable',
  'name_scope',
  'prevent_feeding',
  'prevent_fetching',
  'seed',
  'switch_to_thread_local',
  'unique_name',
  'version'],
 ['colocation_groups',
  'control_inputs',
  'device',
  'get_attr',
  'graph',
  'inputs',
  'name',
  'node_def',
  'op_def',
  'outputs',
  'run',
  'traceback',
  'traceback_with_start_lines',
  'type',
  'values'],
 ['capitalize',
  'casefold',
  'center',
  'count',
  'encode',
  'endswith',
  'expandtabs',
  'find',
  'format',
  'format_map',
  'index',
  'isalnum',
  'isalpha',
  'isdecimal',
  'isdigit',
  'isidentifier',
  'islower',
  'isnumeric',
  'isprintable',
  'isspace',
  'istitle',
  'isupper',
  'join',
  'ljust',
  'lower',
  'lstrip',
  'maketrans',
  'partition',
  'replace',
  'rfind',
  'rindex',
  'rjust',
  'rpartition',
  'rsplit',
  'rstrip',
  'split',
  'splitlines',
  'startswith',
  'strip',
  'swapcase',
  'title',
  'translate',
  'upper',
  'zfill']]
#+END_src

One that immediately strikes some interest is ~ans.op~. Looking at it
the entries ~input~ and ~output~ could be what we are looking for!
Looking closer at these two.

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [ans.op.inputs._inputs, ans.op.outputs]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
[[<tf.Tensor 'MatMul_12:0' shape=(10, 10) dtype=float64>,
  <tf.Tensor 'MatMul_14:0' shape=(10, 10) dtype=float64>],
 [<tf.Tensor 'sub_4:0' shape=(10, 10) dtype=float64>]]
#+END_src

Here it looks like we have references to the previous tensors that
were subtracted to create the answer we got above and the resulting
output. Let's dig a little deeper yet into "ans.op.inputs" and see if
we can glean more information.

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [[i for i in dir(ans.op.inputs._inputs[0]) if not i.startswith('_')],
       [i for i in dir(ans.op.inputs._inputs[1]) if not i.startswith('_')]]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
[['OVERLOADABLE_OPERATORS',
  'consumers',
  'device',
  'dtype',
  'eval',
  'get_shape',
  'graph',
  'name',
  'op',
  'set_shape',
  'shape',
  'value_index'],
 ['OVERLOADABLE_OPERATORS',
  'consumers',
  'device',
  'dtype',
  'eval',
  'get_shape',
  'graph',
  'name',
  'op',
  'set_shape',
  'shape',
  'value_index']]
#+END_src

Looking at the ~dir~ object gives us a lot to look at. For example
let's look at the eval object for these inputs.


#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [ans.op.inputs._inputs[0].eval(), ans.op.inputs._inputs[1].eval()]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
[array([[11.4738134 ,  4.26016144,  0.6781197 ,  2.44359888,  5.74114084,
        -5.08738765,  0.51469722, -1.44799447,  3.84346746, -3.82986559],
       [ 4.26016144,  9.92049161,  2.67004972,  1.15531941,  1.86485298,
        -2.47732948, -3.93827103,  1.03626778,  2.97465137, -4.91821685],
       [ 0.6781197 ,  2.67004972,  6.7600335 , -3.506919  , -0.56250284,
         2.13570519, -6.23690503,  0.60437777,  0.55511039, -2.00723535],
       [ 2.44359888,  1.15531941, -3.506919  , 14.40337668,  0.58971477,
         3.55243256,  0.83760618, -6.93702187, -3.33646066,  1.80625177],
       [ 5.74114084,  1.86485298, -0.56250284,  0.58971477,  7.16490006,
        -2.83154033, -0.87009445, -1.78505806,  2.81836819, -2.02490526],
       [-5.08738765, -2.47732948,  2.13570519,  3.55243256, -2.83154033,
         8.58957555, -1.63243626, -3.76186941, -3.47913535,  0.62505777],
       [ 0.51469722, -3.93827103, -6.23690503,  0.83760618, -0.87009445,
        -1.63243626, 12.96962309,  1.16933893, -0.16354281, -2.62201206],
       [-1.44799447,  1.03626778,  0.60437777, -6.93702187, -1.78505806,
        -3.76186941,  1.16933893,  8.3366981 ,  3.37379237, -2.46187474],
       [ 3.84346746,  2.97465137,  0.55511039, -3.33646066,  2.81836819,
        -3.47913535, -0.16354281,  3.37379237,  5.20192585, -3.82301366],
       [-3.82986559, -4.91821685, -2.00723535,  1.80625177, -2.02490526,
         0.62505777, -2.62201206, -2.46187474, -3.82301366,  8.92444935]]),
 array([[11.4738134 ,  4.26016144,  0.6781197 ,  2.44359888,  5.74114084,
        -5.08738765,  0.51469722, -1.44799447,  3.84346746, -3.82986559],
       [ 4.26016144,  9.92049161,  2.67004972,  1.15531941,  1.86485298,
        -2.47732948, -3.93827103,  1.03626778,  2.97465137, -4.91821685],
       [ 0.6781197 ,  2.67004972,  6.7600335 , -3.506919  , -0.56250284,
         2.13570519, -6.23690503,  0.60437777,  0.55511039, -2.00723535],
       [ 2.44359888,  1.15531941, -3.506919  , 14.40337668,  0.58971477,
         3.55243256,  0.83760618, -6.93702187, -3.33646066,  1.80625177],
       [ 5.74114084,  1.86485298, -0.56250284,  0.58971477,  7.16490006,
        -2.83154033, -0.87009445, -1.78505806,  2.81836819, -2.02490526],
       [-5.08738765, -2.47732948,  2.13570519,  3.55243256, -2.83154033,
         8.58957555, -1.63243626, -3.76186941, -3.47913535,  0.62505777],
       [ 0.51469722, -3.93827103, -6.23690503,  0.83760618, -0.87009445,
        -1.63243626, 12.96962309,  1.16933893, -0.16354281, -2.62201206],
       [-1.44799447,  1.03626778,  0.60437777, -6.93702187, -1.78505806,
        -3.76186941,  1.16933893,  8.3366981 ,  3.37379237, -2.46187474],
       [ 3.84346746,  2.97465137,  0.55511039, -3.33646066,  2.81836819,
        -3.47913535, -0.16354281,  3.37379237,  5.20192585, -3.82301366],
       [-3.82986559, -4.91821685, -2.00723535,  1.80625177, -2.02490526,
         0.62505777, -2.62201206, -2.46187474, -3.82301366,  8.92444935]])]
#+END_src

These both look like our two inputs! We of course directly check this.

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [ans.op.inputs._inputs[0] == S, ans.op.inputs._inputs[1] == D]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
[True, True]
#+END_src

Great! So as a quick recap we now have a way to take the result ~ans~
from our initial statement above and we have found way a way to walk
backwards and get our original matrices. Quickly references the second
question from the beginning, is it possible to determine what kind of
operations are transpiring? For example is it possible to determine if
there was an SVD operation? The quick answer is yes and all we need to
do is use the same methods we've used thus far!

#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = ans.op.inputs._inputs[1].op.inputs._inputs[0].name
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
'Svd_4:1'
#+END_src


 
From here it's clear that there exists a way to move between
operations and from this it is possible to make a graph of operations
with the ability to determine what operations took place. Having this
graph for larger scale projects would allow for better methods of
optimization. How one may ask? Well let's introduce the concept of
goals that would help make this possible. First though we must talk
about the concepts of unify and reify.


** Unify
The idea behind unify is to take two similar terms and form a
*substitution* which can be thought of as a mapping between variables
and values. Let's look at a few quick examples,

| Constant | Variable | Substitution |
| (4, 5)   | (x, 5)   | {x: 4}       |
| 'test'   | 'txst'   | {x: 'e'}     |

In layman's terms at this point we are looking for effectively the set
of values that make the statement true. Below are some examples of
terms that do not unify,

| Constant | Variable | Substitution |
| (4, 5)   | (3, x)   | NA           |
| 'test'   | 'exror'  | NA           |

** Reify
Reify is the opposite operation to unify. This implies that it takes a
variable and a substitution and returns a value that contains no
variables. Below is a quick example,


| Variable | Substitution | Constant |
| (x, 10)  | {x: 5}       | (5, 10)  |
| 'mxsic'  | {x: 'u'}     | 'music'  |

** Goals and there constructors
Using the two concepts above we can now introduce the idea of a
goal. A goal is effectively a stream of substitutions which can be
demonstrated in the following example,

Given that `x is a member of both `(8, 5, 2) and `(5, 2, 9) a stream
of substitutions are {x: 5}, {x: 2}.

** Conclusion
The ability to create graphs and have goals like described allow us to
answer the questions posed at the beginning. In later posts I'll focus
on this idea as it relates to the work I am doing for GSoC.




* work                                                             :noexport:

#+BEGIN_SRC python -n :exports both :results output
  import symbolic_pymc.tensorflow
  from symbolic_pymc.unify import (ExpressionTuple, etuple, tuple_expression)
  z = tuple_expression(ans)
#+END_SRC


# add portion that talks about tuple_expression and unify against that
