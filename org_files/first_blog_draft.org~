#+Title: Unifying Reifying and Symbolic-PyMC
#+Author: Joseph Willard
#+Date: 2019-10-06

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: ^:nil toc:nil d:(not "logbook" "todo" "note" "notes") tex:t |:t broken-links:mark
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+PROPERTY: header-args :session tf :exports both :eval never-export :results output drawer replace
#+PROPERTY: header-args:text :eval never
#+OPTIONS: toc:nil

* Introduction
Digging through tensorflow we started by computing basic examples and
comparing them to numpy's output. While doing this we found that in
many cases there was a common theme of numerical approximation that
theoretically should not have been present. Of course this brought me
to pondering what would we have to do to get around these numerical
errors that arise in software today?

In this article I'll start by walking through the two examples and
show the numerical errors, then I'll muse on what could be done to get
around this and introduce minikanren.

* Motivation
To start Let's look at a basic example using SVD.

#+BEGIN_SRC python -n :results value table
  import numpy as np

  X = np.random.normal(0, 1, (10, 10))
  S = X.T.dot(X)
  U, d, Vt = np.linalg.svd(S)
  _ = S - np.dot(U*d, Vt)
#+END_SRC

#+RESULTS:
:RESULTS:
|  7.10542736e-15 | -6.21724894e-15 |  6.21724894e-15 | -4.88498131e-15 | -2.66453526e-15 | -3.05311332e-15 | -4.88498131e-15 | -5.32907052e-15 |  2.22044605e-15 |  5.99520433e-15 |
|  -8.8817842e-16 |  3.55271368e-15 | -3.64985819e-15 |  3.77475828e-15 |  7.88258347e-15 | -3.66373598e-15 |  5.32907052e-15 |  6.66133815e-15 |   4.4408921e-16 | -6.21724894e-15 |
|  2.66453526e-15 |  2.22044605e-16 |  3.55271368e-15 |  7.77156117e-16 |  2.22044605e-15 |  2.66453526e-15 |  -4.4408921e-15 |             0.0 | -5.32907052e-15 |  -8.8817842e-16 |
| -1.77635684e-15 |   8.8817842e-16 |  3.66373598e-15 |  2.22044605e-15 | -1.33226763e-15 |   4.4408921e-15 |  2.22044605e-15 |  -8.8817842e-16 | -2.49800181e-15 |  -8.8817842e-16 |
|  -8.8817842e-16 | -2.77555756e-15 | -3.99680289e-15 |  1.33226763e-15 |   1.0658141e-14 | -1.77635684e-15 | -5.32907052e-15 |   8.8817842e-16 |  -4.4408921e-16 |  3.55271368e-15 |
| -1.60982339e-15 | -5.21804822e-15 |  6.66133815e-15 |  4.21884749e-15 | -1.77635684e-15 |  7.99360578e-15 |             0.0 |  6.66133815e-16 | -4.88498131e-15 | -1.55431223e-15 |
|  1.33226763e-15 |  1.15463195e-14 | -3.10862447e-15 | -1.77635684e-15 |             0.0 | -2.22044605e-16 |  7.10542736e-15 |  1.36002321e-15 | -2.66453526e-15 | -3.55271368e-15 |
|  -4.4408921e-15 |  -4.4408921e-16 |             0.0 |   8.8817842e-16 | -2.66453526e-15 |  1.99840144e-15 |   2.0539126e-15 |  -8.8817842e-16 |  -8.8817842e-16 |  3.55271368e-15 |
| -3.55271368e-15 |  -8.8817842e-16 |  -8.8817842e-16 | -1.72084569e-15 | -5.88418203e-15 | -2.66453526e-15 |  1.11022302e-15 | -2.66453526e-15 |  1.77635684e-15 |  3.55271368e-15 |
|  5.99520433e-15 | -3.99680289e-15 |  -4.4408921e-15 |  2.66453526e-15 |  3.55271368e-15 | -2.44249065e-15 | -7.99360578e-15 |  7.10542736e-15 |  7.10542736e-15 | -3.55271368e-15 |
:END:

Theoretically, this would raise an eyebrow. Next, does TensorFlow exhibit the same issue?

#+BEGIN_SRC python -n :results value table
  """ Seeing if tensorflow has the same issue
  """
  import tensorflow as tf
  import tensorflow_probability as tfp
  from tensorflow.python.framework.ops import disable_eager_execution


  tf.compat.v1.InteractiveSession()
  disable_eager_execution()

  tfp = tfp.distributions
  X = tfp.Normal(loc=0, scale=1)
  X = X.sample([10, 10])

  S = tf.matmul(X, X, transpose_a=True)

  d, U, V = tf.linalg.svd(S)

  ans = S - tf.matmul(U, tf.matmul(tf.linalg.diag(d), V, adjoint_b=True))

  ans.eval()
#+END_SRC

#+RESULTS:
:RESULTS:
|  7.10542736e-15 | -6.21724894e-15 |  6.21724894e-15 | -4.88498131e-15 | -2.66453526e-15 | -3.05311332e-15 | -4.88498131e-15 | -5.32907052e-15 |  2.22044605e-15 |  5.99520433e-15 |
|  -8.8817842e-16 |  3.55271368e-15 | -3.64985819e-15 |  3.77475828e-15 |  7.88258347e-15 | -3.66373598e-15 |  5.32907052e-15 |  6.66133815e-15 |   4.4408921e-16 | -6.21724894e-15 |
|  2.66453526e-15 |  2.22044605e-16 |  3.55271368e-15 |  7.77156117e-16 |  2.22044605e-15 |  2.66453526e-15 |  -4.4408921e-15 |             0.0 | -5.32907052e-15 |  -8.8817842e-16 |
| -1.77635684e-15 |   8.8817842e-16 |  3.66373598e-15 |  2.22044605e-15 | -1.33226763e-15 |   4.4408921e-15 |  2.22044605e-15 |  -8.8817842e-16 | -2.49800181e-15 |  -8.8817842e-16 |
|  -8.8817842e-16 | -2.77555756e-15 | -3.99680289e-15 |  1.33226763e-15 |   1.0658141e-14 | -1.77635684e-15 | -5.32907052e-15 |   8.8817842e-16 |  -4.4408921e-16 |  3.55271368e-15 |
| -1.60982339e-15 | -5.21804822e-15 |  6.66133815e-15 |  4.21884749e-15 | -1.77635684e-15 |  7.99360578e-15 |             0.0 |  6.66133815e-16 | -4.88498131e-15 | -1.55431223e-15 |
|  1.33226763e-15 |  1.15463195e-14 | -3.10862447e-15 | -1.77635684e-15 |             0.0 | -2.22044605e-16 |  7.10542736e-15 |  1.36002321e-15 | -2.66453526e-15 | -3.55271368e-15 |
|  -4.4408921e-15 |  -4.4408921e-16 |             0.0 |   8.8817842e-16 | -2.66453526e-15 |  1.99840144e-15 |   2.0539126e-15 |  -8.8817842e-16 |  -8.8817842e-16 |  3.55271368e-15 |
| -3.55271368e-15 |  -8.8817842e-16 |  -8.8817842e-16 | -1.72084569e-15 | -5.88418203e-15 | -2.66453526e-15 |  1.11022302e-15 | -2.66453526e-15 |  1.77635684e-15 |  3.55271368e-15 |
|  5.99520433e-15 | -3.99680289e-15 |  -4.4408921e-15 |  2.66453526e-15 |  3.55271368e-15 | -2.44249065e-15 | -7.99360578e-15 |  7.10542736e-15 |  7.10542736e-15 | -3.55271368e-15 |
:END:


In regards to theory this should have been 0, but due to rounding
errors mostly drawn from limitations of floats this is not the case. A
natural question to ask is whether there is a way around this. To
provide an answer to this we need to introduce minikanren/logpy and
the concepts of unify, reify and goals.

# Maybe move to the bottom
Why would we care about this? One reason is of course numerical
accuracy. The other could be this idea of automating "pen and paper"
math. This would allow someone with a domain specific skill set be it
in probability, numerical analysis to be able to automate their
"tricks" and demystify.

Moving back to our issue with SVD above one way to go about
implementing a substitution is to think of the process of doing SVD
above as a graph of operations. To do this however we would need to be
able to traverse the graph, which means that we would need to know
variables parents. Next, let's look at the members of "ans". 


#+BEGIN_SRC python -n :results value pp :wrap "src python :eval never"
  _ = [i for i in dir(ans) if not i.startswith('_')]
#+END_SRC

#+RESULTS:
#+BEGIN_src python :eval never
['OVERLOADABLE_OPERATORS',
 'consumers',
 'device',
 'dtype',
 'eval',
 'get_shape',
 'graph',
 'name',
 'op',
 'set_shape',
 'shape',
 'value_index']
#+END_src

Looking at the dir object we see a few objects of interest, let's print the output of a few of them.

#+BEGIN_SRC python -n 
  print(ans.graph)
  print(ans.op)
  print(ans.name)
#+END_SRC

#+RESULTS:
:RESULTS:
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/tmp/babel-20799dMY/python-20799Eye", line 1, in <module>
    print(ans.graph)
NameError: name 'ans' is not defined
:END:

One that immediately strikes some interest is the output of
"ans.op". Looking at it there is references to two inputs; "MatMul"
and "MatMul_2" could we use this to determine what was subtracted?
Let's look,

#+BEGIN_SRC python -n 
  dir(ans.op)
  list(ans.op.inputs)
  list(ans.op.inputs)
#+END_SRC

#+RESULTS:
[<tf.Tensor 'MatMul:0' shape=(10, 10) dtype=float32>, <tf.Tensor 'MatMul_2:0' shape=(10, 10) dtype=float32>]

Here it looks like we have references to the previous tensors that
were subtracted to create the answer we got above. Let's dig a little
deeper yet into "ans.op".

#+BEGIN_SRC python -n 
  ans.op.op_def
  ans.op.values()
#+END_SRC

From here it's clear that there exists a way to move between
operations and from this it is possible to make a graph of
operations. Having this graph for larger scale projects would allow
for better methods of optimization. How one may ask? Well let's
introduce the concept of goals that would help make this
possible. First though we must talk about the concepts of unify and
reify.


** Unify
The idea behind unify is to take two similar terms and form a
*substitution* which can be thought of as a mapping between variables
and values. Let's look at a few quick examples,

| Constant | Variable | Substitution |
| (4, 5)   | (x, 5)   | {x: 4}       |
| 'test'   | 'txst'   | {x: 'e'}     |

In layman's terms at this point we are looking for effectively the set
of values that make the statement true. Below are some examples of
terms that do not unify,

| Constant | Variable | Substitution |
| (4, 5)   | (3, x)   | NA           |
| 'test'   | 'exror'  | NA           |

** Reify
Reify is the opposite operation to unify. This implies that it takes a
variable and a substitution and returns a value that contains no
variables. Below is a quick example,


| Variable | Substitution | Constant |
| (x, 10)  | {x: 5}       | (5, 10)  |
| 'mxsic'  | {x: 'u'}     | 'music'  |

** Goals and there constructors
Using the two concepts above we can now introduce the idea of a
goal. A goal is effectively a stream of substitutions which can be
demonstrated in the following example,

Given that `x is a member of both `(8, 5, 2) and `(5, 2, 9) a stream
of substitutions are {x: 5}, {x: 2}.


In later posts I'll focus on this idea as it relates to the work I am
doing for GSoC.

* work                                                             :noexport:

#+BEGIN_SRC python -n :exports both :results output
  import symbolic_pymc.tensorflow
  from symbolic_pymc.unify import (ExpressionTuple, etuple, tuple_expression)
  z = tuple_expression(ans)
#+END_SRC
