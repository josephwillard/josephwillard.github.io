#+LaTeX_HEADER: \usepackage{amsmath, amsfonts, listings, amsthm, mathtools, graphicx, tkz-graph, tikz, outlines, fixmath, marginnote, pdfpages, mathrsfs, mathtools, inputenc, todonotes, placeins, bm}
#+Title: BLog 3
#+AUTHOR: Joseph Willard
#+LaTeX: \setcounter{secnumdepth}{0}
#+LaTeX: \newpage
#+STARTUP: hideblocks indent hidestars
#+OPTIONS: ^:nil toc:nil d:(not "logbook" "todo" "note" "notes") tex:t |:t broken-links:mark
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+PROPERTY: header-args :session tf :exports both :eval never-export :results output drawer replace
#+PROPERTY: header-args:text :eval never
#+OPTIONS: toc:nil

* Introduction
All of my blog posts up to this point have addresed dealing with
tensor flow objects that are transformed into ~symbolic-pymc~
meta objects. The natural question now is how do we start with
~PyMC4~ model and convert it into a ~symbolic-pymc~ object?


* A look into new pymc4 models
As of the date this blog has been posted ~PyMC4~ received a large
update introducing generative models. In previous iterations of
~PyMC4~ conversion would involve trying to pinpoint what tensorflow
object represented the observations. Luckily, with the recent changes
this can be controlled by how the model is created relieving some of
the searching on ~symbolic-pymcs~ part.

Consider the following model,

#+BEGIN_SRC python -n :exports both :results output
  from symbolic_pymc.tensorflow.meta import mt
  from symbolic_pymc.tensorflow.printing import tf_dprint
  from symbolic_pymc.unify import (ExpressionTuple, etuple)


  from tensorflow.python.framework.ops import enable_eager_execution
  from tensorflow.python.eager import def_function
  #enable_eager_execution()

  import numpy as np

  import pymc4 as pm

  from pymc4 import distributions as dist



  @pm.model(keep_return=False)
  def nested_model(intercept, x_coeff, x):
      y = yield dist.Normal("y", mu=intercept + x_coeff.sample() * x, sigma=1.0)
      return y


  @pm.model
  def main_model():
      intercept = yield dist.Normal("intercept", mu=0, sigma=10)
      x = np.linspace(-5, 5, 100)
      x_coeff = dist.Normal("x_coeff", mu=0, sigma=5)
      result = yield nested_model(intercept, x_coeff, x)
      return result


  ret, state = pm.evaluate_model(main_model())
  new_graph = mt(ret)
#+END_SRC

* Conversion Function


* Latest Work


#+BEGIN_SRC python -n :exports both :results output
  from symbolic_pymc.tensorflow.meta import mt
  from symbolic_pymc.tensorflow.printing import tf_dprint

  import numpy as np
  import tensorflow as tf
  import pymc4 as pm
  from pymc4 import distributions as dist

  @pm.model(keep_return=False)
  def nested_model(intercept, x_coeff, x):
      y = yield dist.Normal("y", mu=intercept + x_coeff.sample() * x, sigma=1.0)
      return y

  @pm.model
  def main_model():
      intercept = yield dist.Normal("intercept", mu=0, sigma=10)
      x = np.linspace(-5, 5, 100)
      x_coeff = dist.Normal("x_coeff", mu=0, sigma=5)
      result = yield nested_model(intercept, x_coeff, x)
      return result


  ret, state = pm.evaluate_model(main_model())
  new_graph = mt(ret)
#+END_SRC


#+BEGIN_SRC python -n :exports both :results output
  import pymc4 as pm
  from pymc4 import distributions as dist
  @pm.model(keep_return=False)  # do not keep `norm` in return
  def nested_model(cond):
      norm = yield dist.Normal("n", cond, 1)
      return norm
  @pm.model  # keep_return is True by default
  def main_model():
      norm = yield dist.Normal("n", 0, 1)
      result = yield nested_model(norm, name="a")
      return result
  ret, state = pm.evaluate_model(main_model())
#+END_SRC




* COMMENT Work 

#+BEGIN_SRC python -n :exports both :results output
  from symbolic_pymc.tensorflow.meta import mt
  from symbolic_pymc.tensorflow.printing import tf_dprint

  import numpy as np
  import tensorflow as tf
  import pymc4 as pm

  @pm.model()
  def linreg(n_points=100):
      # These could be priors
      intercept = pm.Normal(mu=0, sigma=10)
      x_coeff = pm.Normal(mu=0, sigma=5)
      x = np.linspace(-5, 5, n_points)

      # This could be an observed term (i.e. specify a likelihood)
      y = pm.Normal(mu=intercept + x_coeff * x, sigma=1.0)


  model = linreg.configure()

  # All the RVs are here, but each produces a graph of its own; which one do we
  # want/use/care about?
  model._forward_context.vars

  tf_dprint(model._forward_context.vars[-1].sample())

  # We can specify which one is the "observed" variable
  y_val = tf.convert_to_tensor(np.linspace(-5.0, 5.0, 100), dtype='float32')  # tf.compat.v1.placeholder('float32', name='y')

  # We specify that here and get a new model object
  model = model.observe(y=y_val)

  # This gives use graphs we know we're interested in (instead of all of them):
  y_obs = model._observations['y']

  # XXX: This isn't the graph for `y`!
  tf_dprint(y_obs)


  # Here's one way to fish-out the sample-space graphs for observed variables:
  def model_to_meta_graphs(model):
      model_names_to_vars = {v.name: v for v in model._forward_context.vars}
      observation_graphs = [mt(v.sample()) for vn, v in model_names_to_vars.items()
			    if vn in model._observations]
      # TODO: We want the observation/observed relationship to show up in the TF
      # graph!  For example, we might want an TF "Observe" Op[Def].
      return observation_graphs


  model_to_meta_graphs(model)

  # Now, we have a way to go from PyMC4 graphs to meta graphs

  #
  #  Get a log-space version of the graph; specifically, the total log-probability/likelihood.
  #  Note: this doesn't require one to define "observed" variables and/or a "likelihood".
  #
  model_logp_fn = model.make_log_prob_function()

  x_coeff_val = tf.convert_to_tensor(1.0, dtype='float32')  # tf.compat.v1.placeholder('float32', name='x_coeff')
  intercept_val = tf.convert_to_tensor(1.0, dtype='float32')  # tf.compat.v1.placeholder('float32', name='intercept')
  y_val = tf.convert_to_tensor(np.linspace(-5.0, 5.0, 100), dtype='float32')  # tf.compat.v1.placeholder('float32', name='y')

  model_logp = model_logp_fn(x_coeff=x_coeff_val,
			     y=y_val,
			     intercept=intercept_val)

  model_logp
  model_logp
#+END_SRC

